{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad40428-e616-443d-b0a7-aba3f65e93eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9691fb-b7ec-40de-9065-d3938bb2da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "SCRIPT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../scripts\"))\n",
    "if SCRIPT_DIR not in sys.path:\n",
    "    sys.path.append(SCRIPT_DIR)\n",
    "from config import YEARS, RAW_PATH, PROCESSED_PATH, RESULTS_PATH, WDI_INDICATORS, VDEM_VARIABLES, VDEM_DOWNLOAD_URL, TARGET_VARIABLE, BARRO_LEE_FILENAME, VARIANCE_ANALYSIS, CLUSTERING_VARIABLES\n",
    "from load_vdem import load_vdem_subset\n",
    "from load_wdi import load_and_average_wdi_indicator\n",
    "from load_barro_lee import load_barro_lee_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b477a3f-8f56-49b6-81c7-330f5ad3abc6",
   "metadata": {},
   "source": [
    "# 1 Datensatz erstellen\n",
    "\n",
    "Im ersten Schritt werden die Daten aus den verschiedenen Quellen geladen und auf die für die Analyse relevanten Variablen und Jahre gefiltert. Dabei werden V-Dem-, Weltbank- und Barro-Lee-Daten jeweils separat verarbeitet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0583929-52bb-4386-a054-78ca47339a52",
   "metadata": {},
   "source": [
    "## 1.1 V-Dem: Daten laden & filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f7e984-ca18-42f2-af07-6fff46157393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade V-Dem ZIP-Datei herunter ...\n",
      "Entpacke ZIP-Datei ...\n",
      "Lade und filtere CSV-Datei ...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vdem_df, vdem_variances_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_vdem_subset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43myears\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYEARS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVDEM_VARIABLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVDEM_DOWNLOAD_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRAW_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPROCESSED_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvdem_subset_filtered.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_variance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVARIANCE_ANALYSIS\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Selbststudium\\scripts\\load_vdem.py:59\u001b[0m, in \u001b[0;36mload_vdem_subset\u001b[1;34m(years, variables, download_url, raw_path, processed_path, save_as, compute_variance)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLade und filtere CSV-Datei ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m all_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_text_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m variables\n\u001b[1;32m---> 59\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(years)]\n\u001b[0;32m     62\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry Name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_text_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry Code\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m }, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "vdem_df, vdem_variances_df = load_vdem_subset(\n",
    "    years=YEARS,\n",
    "    variables= VDEM_VARIABLES,\n",
    "    download_url= VDEM_DOWNLOAD_URL,\n",
    "    raw_path= RAW_PATH,\n",
    "    processed_path= PROCESSED_PATH,\n",
    "    save_as=\"vdem_subset_filtered.csv\",\n",
    "    compute_variance = VARIANCE_ANALYSIS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b6b32-ac44-43bb-9e4e-ac3a96ba6f79",
   "metadata": {},
   "source": [
    "## 1.2 WDI: Indikatoren laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdf697-1f41-471a-afdc-886d7d5da003",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_dfs = []\n",
    "metadata_frames = []\n",
    "wdi_variances_dfs = []\n",
    "for wdi_code in WDI_INDICATORS:\n",
    "    # Indikator laden + zugehörige Metadaten holen\n",
    "    df, metadata, variance = load_and_average_wdi_indicator(\n",
    "        wdi_code,\n",
    "        years=YEARS,\n",
    "        raw_path=RAW_PATH,\n",
    "        processed_path=PROCESSED_PATH,\n",
    "        compute_variance = VARIANCE_ANALYSIS\n",
    "    )\n",
    "\n",
    "    # Spalten umbenennen\n",
    "    wdi_dfs.append(df)\n",
    "\n",
    "    metadata_frames.append(metadata)\n",
    "    wdi_variances_dfs.append(variance)\n",
    "\n",
    "# Alle Metadaten zusammenführen\n",
    "metadata_df = pd.concat(metadata_frames, ignore_index=True)\n",
    "metadata_unique = metadata_df.drop_duplicates(subset=[\"Country Code\"])\n",
    "\n",
    "# Speichern\n",
    "meta_path = os.path.join(PROCESSED_PATH, \"Master_Metadata.csv\")\n",
    "metadata_unique.to_csv(meta_path, index=False)\n",
    "print(f\"Master-Metadaten gespeichert unter: {meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d14b79-3bc9-427e-ae69-cd65309a8e95",
   "metadata": {},
   "source": [
    "## 1.3. Barro-Lee: Durchschnittliche Schuljahre laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854af87-b95a-4a5c-a73e-667114cd2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "barro_df = load_barro_lee_subset(target_year = 2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8335f51-61a2-4a68-9dd1-18886fb45233",
   "metadata": {},
   "source": [
    "## 1.4. Varianzen berechnen\n",
    "Hintergrund: Beobachtungszeitraum & Varianzbewertung\n",
    "Ziel dieses Blocks ist es, einen geeigneten Beobachtungszeitraum (z. B. 5 oder 10 Jahre) festzulegen, der zwischen Datenverfügbarkeit und Aussagekraft eines Mittelwerts abwägt. Ein längerer Zeitraum liefert tendenziell vollständigere Daten, erhöht aber auch die Varianz innerhalb der Werte, wodurch der Mittelwert weniger repräsentativ werden kann.\n",
    "\n",
    "Deshalb wird hier die durchschnittliche Varianz je Variable berechnet.\n",
    "Die Konfiguration (YEARS) wurde dabei manuell angepasst, und das Notebook zweimal ausgeführt– einmal für 5 Jahre und einmal für 10 Jahre.\n",
    "Die berechneten Varianzen dienten ausschließlich zur Einschätzung und Auswahl des Zeitraums, nicht für die weitere Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d90a6a-e168-409e-9c01-08765b16ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VARIANCE_ANALYSIS:\n",
    "    # Alle Varianz-DataFrames (WDI + V-Dem) zu einer Übersicht zusammenführen\n",
    "    df_var_summary = pd.concat(wdi_variances_dfs + [vdem_variances_df], ignore_index=True)\n",
    "\n",
    "    # Nach mittlerer Varianz über Länder sortieren\n",
    "    df_var_summary = df_var_summary.sort_values(\"Median Varianz über Länder\", ascending=True)\n",
    "\n",
    "    # Ergebnis als CSV-Datei speichern\n",
    "    filename = f\"df_var_summary_{min(YEARS)}_{max(YEARS)}.csv\"\n",
    "    output_path = os.path.join(RESULTS_PATH, filename)\n",
    "    df_var_summary.to_csv(output_path, index=False)\n",
    "    print(f\"Varianz-Zusammenfassung gespeichert unter: {output_path}\")\n",
    "\n",
    "    # Berechnung und Ausgabe der Gesamt-Medianvarianz zur Einordnung\n",
    "    gesamt_median = df_var_summary[\"Median Varianz über Länder\"].median()\n",
    "    print(f\"Gesamter Median der Länder-Varianzen über alle Variablen: {gesamt_median:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c2557-9597-404f-ae3b-33d4834babc5",
   "metadata": {},
   "source": [
    "## 1.5 Datensätze mergen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b34ad4-60cb-4c9c-8bf1-1e0666c806e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country-Namen aus V-DEM-Frame extrahieren\n",
    "country_names = vdem_df[[\"Country Code\", \"Country Name\"]].drop_duplicates()\n",
    "\n",
    "# WDI-Frames ohne \"Country Name\" vorbereiten, um Mergen zu erleichtern\n",
    "wdi_clean = [df.drop(columns=[\"Country Name\"], errors=\"ignore\") for df in wdi_dfs]\n",
    "\n",
    "# Weltbankdaten zu Hauptdatensatz mergen\n",
    "df_master = reduce(lambda left, right: pd.merge(left, right, on=\"Country Code\", how=\"outer\"), wdi_clean)\n",
    "\n",
    "# Barro-Lee vorbereiten \n",
    "barro_df_clean = barro_df.drop(columns=[\"Country Name\"], errors=\"ignore\")\n",
    "\n",
    "# Barro-Lee in Hauptdatensatz mergen\n",
    "df_master = pd.merge(df_master, barro_df_clean, on=\"Country Code\", how=\"outer\")\n",
    "\n",
    "# Metadata vorbereiten \n",
    "metadata_df_clean = metadata_df.drop(columns=[\"TableName\",\"SpecialNotes\"], errors=\"ignore\")\n",
    "\n",
    "# Metadata in Hauptdatensatz mergen\n",
    "df_master = pd.merge(df_master, metadata_df_clean, on=\"Country Code\", how=\"outer\")\n",
    "\n",
    "# V-Dem vorbereiten \n",
    "vdem_df_clean = vdem_df.drop(columns=[\"Country Name\"], errors=\"ignore\")\n",
    "\n",
    "# V-Dem in Hauptdatensatz mergen(hier werden nur Länder mit existierendem Wert für die Zielvariable verwendet)\n",
    "vdem_valid = vdem_df_clean[vdem_df_clean[\"v2x_partipdem\"].notna()]\n",
    "df_master = pd.merge(df_master, vdem_valid, on=\"Country Code\", how=\"inner\")\n",
    "\n",
    "# Country Name wieder anhängen\n",
    "df_master = pd.merge(country_names, df_master, on=\"Country Code\", how=\"right\")\n",
    "\n",
    "# Entferne leere \"Unnamed\"-Spalten\n",
    "df_master = df_master.loc[:, ~df_master.columns.str.contains(\"^Unnamed\")]\n",
    "df_master = df_master.drop_duplicates(subset=[\"Country Code\"])\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(f\"Finale Länderanzahl: {df_master.shape[0]}\")\n",
    "print(f\"Spaltenanzahl: {df_master.shape[1]}\")\n",
    "\n",
    "# Speichern\n",
    "output_path = os.path.join(RESULTS_PATH, \"dataset_merged.csv\")\n",
    "df_master.to_csv(output_path, index=False)\n",
    "print(f\"Master-Datensatz gespeichert unter: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ac20b-0fc7-4af1-9a34-ec2b3ca83de4",
   "metadata": {},
   "source": [
    "# 2 Preprocessing der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1dd3c8-6bdc-4680-aecb-c09ba8c11713",
   "metadata": {},
   "source": [
    "## 2.1 Analyse fehlender Werte und Datenabdeckung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c869354-9824-465b-8def-876bbf0ede81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anteil fehlender Werte je Variable\n",
    "missing_per_variable = df_master.isna().mean().sort_values(ascending=False)\n",
    "print(missing_per_variable.head(10))\n",
    "\n",
    "# Anteil fehlener Werte je Land\n",
    "df_tmp = df_master.set_index(\"Country Code\")\n",
    "missing_per_country = 1 - (df_tmp.notna().sum(axis=1) / df_tmp.shape[1])\n",
    "missing_per_country = missing_per_country.sort_values(ascending=False)\n",
    "print(missing_per_country.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4601c-01c6-4fc3-a581-9eb1dfea5743",
   "metadata": {},
   "source": [
    "## 2.2 Random Forest Imputation fehlender Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bfbc9-77eb-4897-8eb3-98912e163b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taiwan wurde aufgrund vollständig fehlender sozioökonomischer Kontextdaten (Weltbankindikatoren) aus der Analyse ausgeschlossen.\n",
    "df_master = pd.read_csv(\"../results/dataset_merged.csv\")\n",
    "df_master = df_master[df_master[\"Country Code\"] != \"TWN\"] \n",
    "\n",
    "# Income Groupe von Venezuela Manuell zu Upper-middle-income setzen, da fehlend in Datensatz\n",
    "df_master.loc[df_master[\"Country Code\"] == \"VEN\", \"IncomeGroup\"] = \"Upper middle income\"\n",
    "\n",
    "# Spalten mit fehlenden numerischen Werten finden\n",
    "numerical_cols = df_master.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "cols_with_na = [col for col in numerical_cols if df_master[col].isna().sum() > 0]\n",
    "\n",
    "# Imputer initialisieren \n",
    "imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Nur numerische Spalten mit NaNs imputieren\n",
    "df_imputed_values = imputer.fit_transform(df_master[cols_with_na])\n",
    "\n",
    "# In DataFrame umwandeln\n",
    "df_imputed = pd.DataFrame(df_imputed_values, columns=cols_with_na, index=df_master.index)\n",
    "\n",
    "# Original DataFrame aktualisieren\n",
    "df_master.loc[:, cols_with_na] = df_imputed\n",
    "\n",
    "# 5. Zwischenergebnis speichern\n",
    "output_path = os.path.join(RESULTS_PATH, \"dataset_imputed.csv\")\n",
    "df_master.to_csv(output_path, index=False)\n",
    "print(f\"Imputation abgeschlossen und gespeichert unter: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b999b-daab-417f-9573-675945814ab6",
   "metadata": {},
   "source": [
    "## 2.3 Erneute Analyse fehlender Werte und Datenabdeckung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555be6a-f232-43bf-ad07-d5eade8bdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anteil fehlender Werte je Variable\n",
    "missing_per_variable = df_master.isna().mean().sort_values(ascending=False)\n",
    "print(missing_per_variable.head(10))\n",
    "\n",
    "# Anteil fehlener Werte je Land\n",
    "df_tmp = df_master.set_index(\"Country Code\")\n",
    "missing_per_country = 1 - (df_tmp.notna().sum(axis=1) / df_tmp.shape[1])\n",
    "missing_per_country = missing_per_country.sort_values(ascending=False)\n",
    "print(missing_per_country.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93f785-6d27-4c53-8575-00040caba2c6",
   "metadata": {},
   "source": [
    "# 3 Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ec7db-dc39-4759-8564-7d09a1e8da93",
   "metadata": {},
   "source": [
    "## 3.1 Korrelationsanalyse \n",
    "\n",
    "Bevor ein Modell zur Bestimmung der Einflussfaktoren auf die partizipative Demokratie (Zielvariable: v2x_partipdem) erstellt wird, ist es wichtig zu prüfen, ob einige der potenziellen Prädiktoren stark miteinander korrelieren. Hohe Korrelationen zwischen erklärenden Variablen (Multikollinearität) können Modellverzerrungen verursachen und die Interpretation erschweren.\n",
    "\n",
    "In diesem Schritt werden daher:\n",
    "- alle numerischen Variablen außer der Zielvariable extrahiert,\n",
    "- eine Korrelationsmatrix auf Basis der absoluten Pearson-Korrelation berechnet,\n",
    "- stark korrelierte Paare (Schwellenwert: > 0.85) identifiziert und tabellarisch ausgegeben,\n",
    "- eine visuelle Heatmap zur Übersicht aller Korrelationen dargestellt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c3f2c-14f8-497b-bfaf-cfcf66acf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_master.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "feature_cols = [col for col in feature_cols if col != TARGET_VARIABLE]\n",
    "df_feat = df_master[feature_cols]\n",
    "\n",
    "# Korrelationsmatrix erstellen\n",
    "corr_matrix = df_feat.corr().abs()\n",
    "\n",
    "# Korrelationsanalyse\n",
    "high_corr_df = (\n",
    "    corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: \"Korrelationskoeffizient\"})\n",
    "    .query(\"Korrelationskoeffizient > 0.85\")\n",
    "    .sort_values(by=\"Korrelationskoeffizient\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Stark korrelierte Variablenpaare (Korrelationskoeffizient > 0.85) :\")\n",
    "print(high_corr_df.to_string(index=False))\n",
    "\n",
    "# Heatmap anzeigen und speichern\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", annot=False, center=0)\n",
    "plt.title(\"Korrelationsmatrix\")\n",
    "plt.savefig(os.path.join(RESULTS_PATH, \"korrelationsmatrix.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13449403-513b-4419-af50-3152f89a3b06",
   "metadata": {},
   "source": [
    "## 3.2 Globale Feature Importance Analyse \n",
    "\n",
    "In diesem Schritt wird ein Random-Forest-Modell verwendet, um die relative Bedeutung (Feature Importance) aller potenziellen Einflussfaktoren auf den Partizipationsindex (v2x_partipdem) zu bestimmen. Die Analyse erfolgt global, also über alle Länder hinweg. Die resultierenden Wichtigkeitswerte geben an, welche Merkmale den größten Beitrag zur Erklärung der Zielvariable leisten.\n",
    "Basierend auf diesen Werten können irrelevante oder wenig aussagekräftige Variablen identifiziert und aus der weiteren Analyse ausgeschlossen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd347e49-0582-49bd-ab8f-dce63b86506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabe-Features (X) und Zielvariable (y) definieren\n",
    "X = df_master[feature_cols].copy()\n",
    "y = df_master[TARGET_VARIABLE].copy()\n",
    "\n",
    "# Random Forest Modell initialisieren und trainieren\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Feature Importance berechnen und in DataFrame umwandeln\n",
    "importances = pd.DataFrame({\n",
    "    \"Feature\": feature_cols,\n",
    "    \"Importance\": model.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\n Feature Importance (Random Forest für v2x_partipdem):\")\n",
    "print(importances.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e21d6f-bb21-4cd9-bbbe-f02afbc46d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diese vier Variablen werden wegen starker Korrelation mit andern Variablen und auf Basis der Feature Importance entfernt\n",
    "vars_to_drop = [\"v2x_rule\", \"v2x_corr\", \"v2dlconslt\", \"v2xlg_legcon\", \"SH.XPD.CHEX.PC.CD\"]\n",
    "\n",
    "df_cleaned = df_master.drop(columns=vars_to_drop)\n",
    "df_cleaned.to_csv(os.path.join(RESULTS_PATH, \"dataset_cleaned.csv\"), index=False)\n",
    "print(\"Datensatz ohne stark korrelierte Variablen gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003ca51-10b3-49f8-b9e6-a4bbe6709210",
   "metadata": {},
   "source": [
    "## 3.3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071a312-cc7c-4cde-b385-d180fe2c305e",
   "metadata": {},
   "source": [
    "### 3.3.1 Skalieren\n",
    "\n",
    "Um sicherzustellen, dass alle Einflussfaktoren im Clustering möglichst gleich gewichtet werden, werden die ausgewählten numerischen Variablen mittels Standardisierung (StandardScaler) auf Mittelwert = 0 und Standardabweichung = 1 gebracht. Dies verhindert, dass Variablen mit größeren Wertebereichen (z. B. BIP) den Clustering-Prozess dominieren. Die beiden Variablen Inflation und Bevölkerungszahl wurden vom Clustering ausgeschlossen, da sie sich als potenzielle Verzerrungsfaktoren erwiesen – etwa durch extreme Ausreißer (Inflation) oder enorm unterschiedliche Skalenniveaus (Bevölkerung), die selbst nach Standardisierung zu einer unangemessenen Gewichtung im Clustering führen könnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d25a14-39eb-4aa9-beed-0dae001ad6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalieren\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(df_cleaned[CLUSTERING_VARIABLES])\n",
    "df_scaled = pd.DataFrame(scaled_values, columns=[f\"{col}\" for col in CLUSTERING_VARIABLES])\n",
    "\n",
    "# Kontextspalten beibehalten \n",
    "context_cols = [\"Country Code\", \"Country Name\", \"Region\", \"IncomeGroup\"]\n",
    "for col in context_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_scaled[col] = df_cleaned[col]\n",
    "output_path = os.path.join(RESULTS_PATH, \"cluster_input_scaled.csv\")\n",
    "df_scaled.to_csv(output_path, index=False)\n",
    "print(f\" Skalierte Variablen gespeichert unter: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476309fb-71da-434d-a65d-5113f4d2452c",
   "metadata": {},
   "source": [
    "### 3.3.2 Anzahl der Cluster bestimmen\n",
    "\n",
    "Um eine geeignete Anzahl an Clustern für die anschließende Gruppierung der Länder zu finden, werden zwei gängige Metriken angewendet: die Elbow-Methode (Inertia) und der Silhouette Score. Beide helfen dabei abzuschätzen, bei welcher Clusteranzahl eine sinnvolle Balance zwischen Homogenität innerhalb der Cluster und Trennschärfe zwischen den Clustern erreicht wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84344e17-5390-4bd0-8182-015c79fb9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Auswahl (alle numerischen Spalten ohne Kontextspalten)\n",
    "context_cols = [\"Country Code\", \"Country Name\", \"Region\", \"IncomeGroup\"]\n",
    "feature_cols = [col for col in df_scaled.columns if col not in context_cols]\n",
    "\n",
    "X = df_scaled[feature_cols]\n",
    "\n",
    "# Cluster-Anzahl-Bereich\n",
    "k_range = range(2, 11)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow Plot\n",
    "ax[0].plot(k_range, inertias, marker=\"o\")\n",
    "ax[0].set_xlabel(\"Anzahl der Cluster (k)\")\n",
    "ax[0].set_ylabel(\"Inertia (Summe der quadrierten Distanzen)\")\n",
    "ax[0].set_title(\"Elbow-Methode\")\n",
    "\n",
    "# Silhouette Plot\n",
    "ax[1].plot(k_range, silhouette_scores, marker=\"o\", color=\"orange\")\n",
    "ax[1].set_xlabel(\"Anzahl der Cluster (k)\")\n",
    "ax[1].set_ylabel(\"Silhouette Score\")\n",
    "ax[1].set_title(\"Silhouette Scores\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, \"elbow_und_silhouette.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd79255-23d3-44d2-8f4a-f41b37889531",
   "metadata": {},
   "source": [
    "### 3.3.3 Clustering durchführen\n",
    "\n",
    "Die Elbow-Methode zeigt einen deutlichen Knick bei k=4, was auf eine sinnvolle Clusteranzahl hinweist. Auch der Silhouettenkoeffizient erreicht bei vier Clustern einen lokalen Höchstwert (ca. 0,39). Auf dieser Grundlage wurde für die weitere Analyse die Gruppierung in vier Cluster gewählt.\n",
    "\n",
    "Die Zuordnung zu den Clustern erfolgt nun mittels des KMeans-Algorithmus basierend auf den skalierten sozioökonomischen Variablen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e746d6-cdd5-493f-a002-1b87612b08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans-Modell initialisieren und fitten\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=\"auto\")\n",
    "df_scaled[\"cluster\"] = kmeans.fit_predict(df_scaled[CLUSTERING_VARIABLES])\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(\"Cluster-Zuweisung abgeschlossen.\")\n",
    "\n",
    "# Abspeichern\n",
    "output_path = os.path.join(RESULTS_PATH, \"dataset_clustered.csv\")\n",
    "df_scaled.to_csv(output_path, index=False)\n",
    "print(f\"Ergebnis gespeichert unter: {output_path}\")\n",
    "\n",
    "print(\"Anzahl der Länder pro Cluster:\")\n",
    "print(df_scaled[\"cluster\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e3720-8823-4be5-a74f-c30bbeb2f7a9",
   "metadata": {},
   "source": [
    "Die Cluster sind unterschiedlich groß. Jedes Cluster in sich ist jedoch groß genug, um damit weitere Analysen machen zu können\n",
    "\n",
    "Bevor die einzelnen Cluster im Detail analysiert und interpretiert werden, erfolgt zunächst eine Visualisierung der Clusterstruktur. Hierzu wurde eine Hauptkomponentenanalyse (PCA) durchgeführt, um die hochdimensionalen Daten auf zwei Dimensionen zu reduzieren. Die folgende Darstellung zeigt die räumliche Verteilung der Länder im PCA-Raum, farblich differenziert nach ihrer jeweiligen Clusterzugehörigkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15052dfd-dd43-4b4a-8e7c-21f4f3dea719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptkomponentenanalyse (PCA) mit zwei Komponenten zur Visualisierung\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(df_scaled[feature_cols])\n",
    "df_scaled[\"PCA1\"], df_scaled[\"PCA2\"] = components[:,0], components[:,1]\n",
    "\n",
    "# Scatter-Plot zur Darstellung der Cluster im zweidimensionalen PCA-Raum\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in sorted(df_scaled[\"cluster\"].unique()):\n",
    "    subset = df_scaled[df_scaled[\"cluster\"] == cluster]\n",
    "    plt.scatter(subset[\"PCA1\"], subset[\"PCA2\"], label=f\"Cluster {cluster}\")\n",
    "plt.legend()\n",
    "plt.title(\"Cluster-Verteilung in PCA-Raum\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.savefig(os.path.join(RESULTS_PATH, \"pca_cluster.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af66aab-7884-46e3-a729-aaddce371b7f",
   "metadata": {},
   "source": [
    "Die Visualisierung im PCA-Raum zeigt eine klare Trennung der vier Cluster. Auch wenn es ein paar Überschneidungen gibt, bestätigt die Projektion insgesamt die strukturelle Trennbarkeit der Gruppen im mehrdimensionalen Raum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b07c8-a08e-4e1d-8daa-98d15bf1c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesamter Silhouettenkoeffizient\n",
    "sil_score = silhouette_score(df_scaled[CLUSTERING_VARIABLES], df_scaled[\"cluster\"])\n",
    "print(f\"Silhouettenkoeffizient für k=4: {sil_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af000a-8261-40f4-a915-f54800e7c9c9",
   "metadata": {},
   "source": [
    "Der durchschnittliche Silhouettenkoeffizient für die gewählte Clustering-Lösung mit vier Gruppen beträgt 0,387. Dieser Wert deutet auf eine ausreichende Trennschärfe zwischen den Clustern hin und unterstützt die Entscheidung für k=4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b0a8e-6d3e-4076-8618-627f2369df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster Zentren:\")\n",
    "print(pd.DataFrame(kmeans.cluster_centers_, columns=CLUSTERING_VARIABLES))\n",
    "\n",
    "# Verteilung der Einkommensgruppen in den Clustern\n",
    "print(\"Verteilung der Einkommensgruppen pro Cluster:\")\n",
    "print(pd.crosstab(df_scaled[\"cluster\"], df_scaled[\"IncomeGroup\"]), \"\\n\")\n",
    "\n",
    "# Verteilung der Weltregionen in den Clustern\n",
    "print(\"Verteilung der Weltregionen pro Cluster:\")\n",
    "print(pd.crosstab(df_scaled[\"cluster\"], df_scaled[\"Region\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b6085-9e1e-4c16-96b8-55f8731fc284",
   "metadata": {},
   "source": [
    "Beschreibung der Cluster\n",
    "\n",
    "\n",
    "Die vier ermittelten Cluster lassen sich inhaltlich gut voneinander abgrenzen:\n",
    "\n",
    "- Cluster 0: Wirtschaftlich und infrastrukturell gemischte Länder mit höherer Ungleichheit. Hierzu zählen viele lateinamerikanische und MENA-Staaten sowie einige transformierende Volkswirtschaften.\n",
    "- Cluster 1: Staaten mit niedrigem Entwicklungsstand, schwacher Infrastruktur und geringer Bildungsbeteiligung. Stark vertreten: Länder mit niedrigem Einkommen, vor allem in Subsahara-Afrika und Südasien.\n",
    "- - Cluster 2: Hoch entwickelte Staaten mit sehr hohem BIP, hoher Bildung, starker Digitalisierung und vergleichsweise geringer Ungleichheit. Dominieren: High-Income-Staaten, vor allem in Europa und Nordamerika.\n",
    "- Cluster 3: Bildungsstarke Länder mit mittlerem Einkommen, solider Infrastruktur und ebenfalls relativ geringer Ungleichheit. Auffällig viele Staaten aus Europa und Zentralasien.\n",
    "\n",
    "\n",
    "Diese Differenzierung bildet eine solide Grundlage für die anschließende Analyse partizipativer Demokratie im Kontext struktureller Bedingungen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43823521-5219-410b-a395-f4a11a747a4c",
   "metadata": {},
   "source": [
    "## 3.4 Feature-Importance-Analyse innerhalb der Cluster\n",
    "\n",
    "Zunächst wird ein Random-Forest-Modell auf dem Gesamtdatensatz trainiert, um die wichtigsten Einflussfaktoren auf die partizipative Demokratie zu identifizieren. Anschließend erfolgt dieselbe Analyse für jedes Cluster separat, um Unterschiede im Einfluss struktureller Merkmale zwischen Ländergruppen sichtbar zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e857ebb-96eb-4eec-a992-b0733102c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_cleaned.merge(df_scaled[[\"Country Code\", \"cluster\"]], on=\"Country Code\", how=\"left\")\n",
    "\n",
    "# Zielvariable & Kontext\n",
    "cluster_col = \"cluster\"\n",
    "\n",
    "# Features: alle numerischen Spalten außer der Zielvariable\n",
    "exclude_cols = [\"Country Code\", \"Country Name\", \"Region\", \"IncomeGroup\", TARGET_VARIABLE, cluster_col]\n",
    "feature_cols = [col for col in df_final.select_dtypes(include=[\"float64\", \"int64\"]).columns if col not in exclude_cols]\n",
    "\n",
    "# Globale Feature Importance (über alle Länder hinweg)\n",
    "print(\"\\n Globale Feature Importance – Gesamtdatensatz\")\n",
    "X_all = df_final[feature_cols]\n",
    "y_all = df_final[TARGET_VARIABLE]\n",
    "\n",
    "rf_global = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_global.fit(X_all, y_all)\n",
    "\n",
    "global_importances = pd.DataFrame({\n",
    "    \"Feature\": feature_cols,\n",
    "    \"Importance\": rf_global.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    data=global_importances.head(15),\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    hue=\"Feature\",\n",
    "    palette=\"viridis\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Globale Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, \"globale_feature_importance.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Feature Importance für jeden Cluster\n",
    "for cluster in sorted(df_final[cluster_col].dropna().unique()):\n",
    "    print(f\"\\n Cluster {cluster} – Länderanzahl: {df_final[df_final[cluster_col] == cluster].shape[0]}\")\n",
    "\n",
    "    # Daten aus Cluster extrahieren\n",
    "    df_cluster = df_final[df_final[cluster_col] == cluster]\n",
    "\n",
    "    # X und y\n",
    "    X = df_cluster[feature_cols]\n",
    "    y = df_cluster[TARGET_VARIABLE]\n",
    "\n",
    "    # Modell trainieren\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # Feature Importance extrahieren\n",
    "    importances = rf.feature_importances_\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"Feature\": feature_cols,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(\n",
    "    data=fi_df.head(15),\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    hue=\"Feature\",\n",
    "    palette=\"viridis\",\n",
    "    legend=False\n",
    ")\n",
    "    plt.title(f\"Top Feature Importances – Cluster {cluster}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, f\"feature_importance_{cluster}.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9c1f1-5ed0-4118-bf2c-7e35df299d0d",
   "metadata": {},
   "source": [
    "Die Visualisierungen der Feature-Importance-Analyse zeigen, dass sich die wichtigsten Einflussgrößen auf partizipative Demokratie je nach Cluster deutlich unterscheiden.\n",
    "\n",
    "Im Gesamtdatensatz dominieren institutionelle Faktoren wie Justizunabhängigkeit (`v2x_jucon`) und Medienfreiheit (`v2mecenefi`) die Erklärung partizipativer Demokratie. Auch gleichheitsbezogene Variablen wie `v2xeg_eqaccess` und `v2dlengage` sind zentral.\n",
    "\n",
    "In Cluster 0 (ungleiche, mittel entwickelte Länder) ist Medienfreiheit der stärkste Einflussfaktor, gefolgt von politischem Zugang und Justizunabhängigkeit. \n",
    "\n",
    "Cluster 1 (niedrig entwickelte Länder) zeigt ein ähnliches Muster: Institutionelle Merkmale wie Justiz, Teilhabe und Gleichheit sind entscheidend, sozioökonomische Faktoren spielen kaum eine Rolle.\n",
    "\n",
    "In Cluster 2 (hoch entwickelte Länder) gewinnen gleichheits- und vielfaltsbezogene Variablen an Bedeutung, während klassische Entwicklungsindikatoren moderat wichtig bleiben.\n",
    "\n",
    "Cluster 3 (bildungsstarke Mittelgruppe) wird ebenfalls stark durch Medienfreiheit, Gleichheit und zivilgesellschaftliche Beteiligung geprägt, während ökonomische Variablen geringere Relevanz aufweisen.\n",
    "\n",
    "Die Unterschiede in den Rangfolgen und den dominanten Einflussgrößen legen nahe, dass partizipative Demokratie in verschiedenen Kontexten durch unterschiedliche Faktoren gefördert oder gehemmt wird."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
