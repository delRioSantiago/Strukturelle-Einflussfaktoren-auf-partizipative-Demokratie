{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad40428-e616-443d-b0a7-aba3f65e93eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9691fb-b7ec-40de-9065-d3938bb2da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "SCRIPT_DIR = os.path.abspath(os.path.join(os.getcwd(), \"../scripts\"))\n",
    "if SCRIPT_DIR not in sys.path:\n",
    "    sys.path.append(SCRIPT_DIR)\n",
    "from config import YEARS, RAW_PATH, PROCESSED_PATH, RESULTS_PATH, WDI_INDICATORS, VDEM_VARIABLES, VDEM_DOWNLOAD_URL, TARGET_VARIABLE, BARRO_LEE_FILENAME, VARIANCE_ANALYSIS, CLUSTERING_VARIABLES\n",
    "from load_vdem import load_vdem_subset\n",
    "from load_wdi import load_and_average_wdi_indicator\n",
    "from load_barro_lee import load_barro_lee_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b477a3f-8f56-49b6-81c7-330f5ad3abc6",
   "metadata": {},
   "source": [
    "# 1 Datensatz erstellen\n",
    "\n",
    "Im ersten Schritt werden die Daten aus den verschiedenen Quellen geladen und auf die fÃ¼r die Analyse relevanten Variablen und Jahre gefiltert. Dabei werden V-Dem-, Weltbank- und Barro-Lee-Daten jeweils separat verarbeitet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0583929-52bb-4386-a054-78ca47339a52",
   "metadata": {},
   "source": [
    "## 1.1 V-Dem: Daten laden & filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f7e984-ca18-42f2-af07-6fff46157393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade V-Dem ZIP-Datei herunter ...\n",
      "Entpacke ZIP-Datei ...\n",
      "Lade und filtere CSV-Datei ...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vdem_df, vdem_variances_df \u001b[38;5;241m=\u001b[39m \u001b[43mload_vdem_subset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43myears\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYEARS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVDEM_VARIABLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVDEM_DOWNLOAD_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRAW_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPROCESSED_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_as\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvdem_subset_filtered.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_variance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVARIANCE_ANALYSIS\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Selbststudium\\scripts\\load_vdem.py:59\u001b[0m, in \u001b[0;36mload_vdem_subset\u001b[1;34m(years, variables, download_url, raw_path, processed_path, save_as, compute_variance)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLade und filtere CSV-Datei ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m all_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_text_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m variables\n\u001b[1;32m---> 59\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mall_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(years)]\n\u001b[0;32m     62\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry Name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry_text_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry Code\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m }, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[1;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "vdem_df, vdem_variances_df = load_vdem_subset(\n",
    "    years=YEARS,\n",
    "    variables= VDEM_VARIABLES,\n",
    "    download_url= VDEM_DOWNLOAD_URL,\n",
    "    raw_path= RAW_PATH,\n",
    "    processed_path= PROCESSED_PATH,\n",
    "    save_as=\"vdem_subset_filtered.csv\",\n",
    "    compute_variance = VARIANCE_ANALYSIS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b6b32-ac44-43bb-9e4e-ac3a96ba6f79",
   "metadata": {},
   "source": [
    "## 1.2 WDI: Indikatoren laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdf697-1f41-471a-afdc-886d7d5da003",
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_dfs = []\n",
    "metadata_frames = []\n",
    "wdi_variances_dfs = []\n",
    "for wdi_code in WDI_INDICATORS:\n",
    "    # Indikator laden + zugehÃ¶rige Metadaten holen\n",
    "    df, metadata, variance = load_and_average_wdi_indicator(\n",
    "        wdi_code,\n",
    "        years=YEARS,\n",
    "        raw_path=RAW_PATH,\n",
    "        processed_path=PROCESSED_PATH,\n",
    "        compute_variance = VARIANCE_ANALYSIS\n",
    "    )\n",
    "\n",
    "    # Spalten umbenennen\n",
    "    wdi_dfs.append(df)\n",
    "\n",
    "    metadata_frames.append(metadata)\n",
    "    wdi_variances_dfs.append(variance)\n",
    "\n",
    "# Alle Metadaten zusammenfÃ¼hren\n",
    "metadata_df = pd.concat(metadata_frames, ignore_index=True)\n",
    "metadata_unique = metadata_df.drop_duplicates(subset=[\"Country Code\"])\n",
    "\n",
    "# Speichern\n",
    "meta_path = os.path.join(PROCESSED_PATH, \"Master_Metadata.csv\")\n",
    "metadata_unique.to_csv(meta_path, index=False)\n",
    "print(f\"Master-Metadaten gespeichert unter: {meta_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d14b79-3bc9-427e-ae69-cd65309a8e95",
   "metadata": {},
   "source": [
    "## 1.3. Barro-Lee: Durchschnittliche Schuljahre laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854af87-b95a-4a5c-a73e-667114cd2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "barro_df = load_barro_lee_subset(target_year = 2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8335f51-61a2-4a68-9dd1-18886fb45233",
   "metadata": {},
   "source": [
    "## 1.4. Varianzen berechnen\n",
    "Hintergrund: Beobachtungszeitraum & Varianzbewertung\n",
    "Ziel dieses Blocks ist es, einen geeigneten Beobachtungszeitraum (z.â¯B. 5 oder 10 Jahre) festzulegen, der zwischen DatenverfÃ¼gbarkeit und Aussagekraft eines Mittelwerts abwÃ¤gt. Ein lÃ¤ngerer Zeitraum liefert tendenziell vollstÃ¤ndigere Daten, erhÃ¶ht aber auch die Varianz innerhalb der Werte, wodurch der Mittelwert weniger reprÃ¤sentativ werden kann.\n",
    "\n",
    "Deshalb wird hier die durchschnittliche Varianz je Variable berechnet.\n",
    "Die Konfiguration (YEARS) wurde dabei manuell angepasst, und das Notebook zweimal ausgefÃ¼hrtâ einmal fÃ¼r 5 Jahre und einmal fÃ¼r 10 Jahre.\n",
    "Die berechneten Varianzen dienten ausschlieÃlich zur EinschÃ¤tzung und Auswahl des Zeitraums, nicht fÃ¼r die weitere Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d90a6a-e168-409e-9c01-08765b16ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if VARIANCE_ANALYSIS:\n",
    "    # Alle Varianz-DataFrames (WDI + V-Dem) zu einer Ãbersicht zusammenfÃ¼hren\n",
    "    df_var_summary = pd.concat(wdi_variances_dfs + [vdem_variances_df], ignore_index=True)\n",
    "\n",
    "    # Nach mittlerer Varianz Ã¼ber LÃ¤nder sortieren\n",
    "    df_var_summary = df_var_summary.sort_values(\"Median Varianz Ã¼ber LÃ¤nder\", ascending=True)\n",
    "\n",
    "    # Ergebnis als CSV-Datei speichern\n",
    "    filename = f\"df_var_summary_{min(YEARS)}_{max(YEARS)}.csv\"\n",
    "    output_path = os.path.join(RESULTS_PATH, filename)\n",
    "    df_var_summary.to_csv(output_path, index=False)\n",
    "    print(f\"Varianz-Zusammenfassung gespeichert unter: {output_path}\")\n",
    "\n",
    "    # Berechnung und Ausgabe der Gesamt-Medianvarianz zur Einordnung\n",
    "    gesamt_median = df_var_summary[\"Median Varianz Ã¼ber LÃ¤nder\"].median()\n",
    "    print(f\"Gesamter Median der LÃ¤nder-Varianzen Ã¼ber alle Variablen: {gesamt_median:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18c2557-9597-404f-ae3b-33d4834babc5",
   "metadata": {},
   "source": [
    "## 1.5 DatensÃ¤tze mergen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b34ad4-60cb-4c9c-8bf1-1e0666c806e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country-Namen aus V-DEM-Frame extrahieren\n",
    "country_names = vdem_df[[\"Country Code\", \"Country Name\"]].drop_duplicates()\n",
    "\n",
    "# WDI-Frames ohne \"Country Name\" vorbereiten, um Mergen zu erleichtern\n",
    "wdi_clean = [df.drop(columns=[\"Country Name\"], errors=\"ignore\") for df in wdi_dfs]\n",
    "\n",
    "# Weltbankdaten zu Hauptdatensatz mergen\n",
    "df_master = reduce(lambda left, right: pd.merge(left, right, on=\"Country Code\", how=\"outer\"), wdi_clean)\n",
    "\n",
    "# Barro-Lee vorbereiten \n",
    "barro_df_clean = barro_df.drop(columns=[\"Country Name\"], errors=\"ignore\")\n",
    "\n",
    "# Barro-Lee in Hauptdatensatz mergen\n",
    "df_master = pd.merge(df_master, barro_df_clean, on=\"Country Code\", how=\"outer\")\n",
    "\n",
    "# Metadata vorbereiten \n",
    "metadata_df_clean = metadata_df.drop(columns=[\"TableName\",\"SpecialNotes\"], errors=\"ignore\")\n",
    "\n",
    "# Metadata in Hauptdatensatz mergen\n",
    "df_master = pd.merge(df_master, metadata_df_clean, on=\"Country Code\", how=\"outer\")\n",
    "\n",
    "# V-Dem vorbereiten \n",
    "vdem_df_clean = vdem_df.drop(columns=[\"Country Name\"], errors=\"ignore\")\n",
    "\n",
    "# V-Dem in Hauptdatensatz mergen(hier werden nur LÃ¤nder mit existierendem Wert fÃ¼r die Zielvariable verwendet)\n",
    "vdem_valid = vdem_df_clean[vdem_df_clean[\"v2x_partipdem\"].notna()]\n",
    "df_master = pd.merge(df_master, vdem_valid, on=\"Country Code\", how=\"inner\")\n",
    "\n",
    "# Country Name wieder anhÃ¤ngen\n",
    "df_master = pd.merge(country_names, df_master, on=\"Country Code\", how=\"right\")\n",
    "\n",
    "# Entferne leere \"Unnamed\"-Spalten\n",
    "df_master = df_master.loc[:, ~df_master.columns.str.contains(\"^Unnamed\")]\n",
    "df_master = df_master.drop_duplicates(subset=[\"Country Code\"])\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(f\"Finale LÃ¤nderanzahl: {df_master.shape[0]}\")\n",
    "print(f\"Spaltenanzahl: {df_master.shape[1]}\")\n",
    "\n",
    "# Speichern\n",
    "output_path = os.path.join(RESULTS_PATH, \"dataset_merged.csv\")\n",
    "df_master.to_csv(output_path, index=False)\n",
    "print(f\"Master-Datensatz gespeichert unter: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ac20b-0fc7-4af1-9a34-ec2b3ca83de4",
   "metadata": {},
   "source": [
    "# 2 Preprocessing der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1dd3c8-6bdc-4680-aecb-c09ba8c11713",
   "metadata": {},
   "source": [
    "## 2.1 Analyse fehlender Werte und Datenabdeckung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c869354-9824-465b-8def-876bbf0ede81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anteil fehlender Werte je Variable\n",
    "missing_per_variable = df_master.isna().mean().sort_values(ascending=False)\n",
    "print(missing_per_variable.head(10))\n",
    "\n",
    "# Anteil fehlener Werte je Land\n",
    "df_tmp = df_master.set_index(\"Country Code\")\n",
    "missing_per_country = 1 - (df_tmp.notna().sum(axis=1) / df_tmp.shape[1])\n",
    "missing_per_country = missing_per_country.sort_values(ascending=False)\n",
    "print(missing_per_country.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4601c-01c6-4fc3-a581-9eb1dfea5743",
   "metadata": {},
   "source": [
    "## 2.2 Random Forest Imputation fehlender Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bfbc9-77eb-4897-8eb3-98912e163b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taiwan wurde aufgrund vollstÃ¤ndig fehlender sozioÃ¶konomischer Kontextdaten (Weltbankindikatoren) aus der Analyse ausgeschlossen.\n",
    "df_master = pd.read_csv(\"../results/dataset_merged.csv\")\n",
    "df_master = df_master[df_master[\"Country Code\"] != \"TWN\"] \n",
    "\n",
    "# Income Groupe von Venezuela Manuell zu Upper-middle-income setzen, da fehlend in Datensatz\n",
    "df_master.loc[df_master[\"Country Code\"] == \"VEN\", \"IncomeGroup\"] = \"Upper middle income\"\n",
    "\n",
    "# Spalten mit fehlenden numerischen Werten finden\n",
    "numerical_cols = df_master.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "cols_with_na = [col for col in numerical_cols if df_master[col].isna().sum() > 0]\n",
    "\n",
    "# Imputer initialisieren \n",
    "imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Nur numerische Spalten mit NaNs imputieren\n",
    "df_imputed_values = imputer.fit_transform(df_master[cols_with_na])\n",
    "\n",
    "# In DataFrame umwandeln\n",
    "df_imputed = pd.DataFrame(df_imputed_values, columns=cols_with_na, index=df_master.index)\n",
    "\n",
    "# Original DataFrame aktualisieren\n",
    "df_master.loc[:, cols_with_na] = df_imputed\n",
    "\n",
    "# 5. Zwischenergebnis speichern\n",
    "output_path = os.path.join(RESULTS_PATH, \"dataset_imputed.csv\")\n",
    "df_master.to_csv(output_path, index=False)\n",
    "print(f\"Imputation abgeschlossen und gespeichert unter: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b999b-daab-417f-9573-675945814ab6",
   "metadata": {},
   "source": [
    "## 2.3 Erneute Analyse fehlender Werte und Datenabdeckung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555be6a-f232-43bf-ad07-d5eade8bdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anteil fehlender Werte je Variable\n",
    "missing_per_variable = df_master.isna().mean().sort_values(ascending=False)\n",
    "print(missing_per_variable.head(10))\n",
    "\n",
    "# Anteil fehlener Werte je Land\n",
    "df_tmp = df_master.set_index(\"Country Code\")\n",
    "missing_per_country = 1 - (df_tmp.notna().sum(axis=1) / df_tmp.shape[1])\n",
    "missing_per_country = missing_per_country.sort_values(ascending=False)\n",
    "print(missing_per_country.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93f785-6d27-4c53-8575-00040caba2c6",
   "metadata": {},
   "source": [
    "# 3 Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ec7db-dc39-4759-8564-7d09a1e8da93",
   "metadata": {},
   "source": [
    "## 3.1 Korrelationsanalyse \n",
    "\n",
    "Bevor ein Modell zur Bestimmung der Einflussfaktoren auf die partizipative Demokratie (Zielvariable: v2x_partipdem) erstellt wird, ist es wichtig zu prÃ¼fen, ob einige der potenziellen PrÃ¤diktoren stark miteinander korrelieren. Hohe Korrelationen zwischen erklÃ¤renden Variablen (MultikollinearitÃ¤t) kÃ¶nnen Modellverzerrungen verursachen und die Interpretation erschweren.\n",
    "\n",
    "In diesem Schritt werden daher:\n",
    "- alle numerischen Variablen auÃer der Zielvariable extrahiert,\n",
    "- eine Korrelationsmatrix auf Basis der absoluten Pearson-Korrelation berechnet,\n",
    "- stark korrelierte Paare (Schwellenwert: > 0.85) identifiziert und tabellarisch ausgegeben,\n",
    "- eine visuelle Heatmap zur Ãbersicht aller Korrelationen dargestellt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5c3f2c-14f8-497b-bfaf-cfcf66acf232",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_master.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "feature_cols = [col for col in feature_cols if col != TARGET_VARIABLE]\n",
    "df_feat = df_master[feature_cols]\n",
    "\n",
    "# Korrelationsmatrix erstellen\n",
    "corr_matrix = df_feat.corr().abs()\n",
    "\n",
    "# Korrelationsanalyse\n",
    "high_corr_df = (\n",
    "    corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    .stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: \"Korrelationskoeffizient\"})\n",
    "    .query(\"Korrelationskoeffizient > 0.85\")\n",
    "    .sort_values(by=\"Korrelationskoeffizient\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Stark korrelierte Variablenpaare (Korrelationskoeffizient > 0.85) :\")\n",
    "print(high_corr_df.to_string(index=False))\n",
    "\n",
    "# Heatmap anzeigen und speichern\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", annot=False, center=0)\n",
    "plt.title(\"Korrelationsmatrix\")\n",
    "plt.savefig(os.path.join(RESULTS_PATH, \"korrelationsmatrix.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13449403-513b-4419-af50-3152f89a3b06",
   "metadata": {},
   "source": [
    "## 3.2 Globale Feature Importance Analyse \n",
    "\n",
    "In diesem Schritt wird ein Random-Forest-Modell verwendet, um die relative Bedeutung (Feature Importance) aller potenziellen Einflussfaktoren auf den Partizipationsindex (v2x_partipdem) zu bestimmen. Die Analyse erfolgt global, also Ã¼ber alle LÃ¤nder hinweg. Die resultierenden Wichtigkeitswerte geben an, welche Merkmale den grÃ¶Ãten Beitrag zur ErklÃ¤rung der Zielvariable leisten.\n",
    "Basierend auf diesen Werten kÃ¶nnen irrelevante oder wenig aussagekrÃ¤ftige Variablen identifiziert und aus der weiteren Analyse ausgeschlossen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd347e49-0582-49bd-ab8f-dce63b86506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabe-Features (X) und Zielvariable (y) definieren\n",
    "X = df_master[feature_cols].copy()\n",
    "y = df_master[TARGET_VARIABLE].copy()\n",
    "\n",
    "# Random Forest Modell initialisieren und trainieren\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Feature Importance berechnen und in DataFrame umwandeln\n",
    "importances = pd.DataFrame({\n",
    "    \"Feature\": feature_cols,\n",
    "    \"Importance\": model.feature_importances_\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\n Feature Importance (Random Forest fÃ¼r v2x_partipdem):\")\n",
    "print(importances.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e21d6f-bb21-4cd9-bbbe-f02afbc46d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diese vier Variablen werden wegen starker Korrelation mit andern Variablen und auf Basis der Feature Importance entfernt\n",
    "vars_to_drop = [\"v2x_rule\", \"v2x_corr\", \"v2dlconslt\", \"v2xlg_legcon\", \"SH.XPD.CHEX.PC.CD\"]\n",
    "\n",
    "df_cleaned = df_master.drop(columns=vars_to_drop)\n",
    "df_cleaned.to_csv(os.path.join(RESULTS_PATH, \"dataset_cleaned.csv\"), index=False)\n",
    "print(\"Datensatz ohne stark korrelierte Variablen gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f003ca51-10b3-49f8-b9e6-a4bbe6709210",
   "metadata": {},
   "source": [
    "## 3.3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071a312-cc7c-4cde-b385-d180fe2c305e",
   "metadata": {},
   "source": [
    "### 3.3.1 Skalieren\n",
    "\n",
    "Um sicherzustellen, dass alle Einflussfaktoren im Clustering mÃ¶glichst gleich gewichtet werden, werden die ausgewÃ¤hlten numerischen Variablen mittels Standardisierung (StandardScaler) auf Mittelwertâ¯=â¯0 und Standardabweichungâ¯=â¯1 gebracht. Dies verhindert, dass Variablen mit grÃ¶Ãeren Wertebereichen (z.â¯B. BIP) den Clustering-Prozess dominieren. Die beiden Variablen Inflation und BevÃ¶lkerungszahl wurden vom Clustering ausgeschlossen, da sie sich als potenzielle Verzerrungsfaktoren erwiesen â etwa durch extreme AusreiÃer (Inflation) oder enorm unterschiedliche Skalenniveaus (BevÃ¶lkerung), die selbst nach Standardisierung zu einer unangemessenen Gewichtung im Clustering fÃ¼hren kÃ¶nnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d25a14-39eb-4aa9-beed-0dae001ad6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalieren\n",
    "scaler = StandardScaler()\n",
    "scaled_values = scaler.fit_transform(df_cleaned[CLUSTERING_VARIABLES])\n",
    "df_scaled = pd.DataFrame(scaled_values, columns=[f\"{col}\" for col in CLUSTERING_VARIABLES])\n",
    "\n",
    "# Kontextspalten beibehalten \n",
    "context_cols = [\"Country Code\", \"Country Name\", \"Region\", \"IncomeGroup\"]\n",
    "for col in context_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_scaled[col] = df_cleaned[col]\n",
    "output_path = os.path.join(RESULTS_PATH, \"cluster_input_scaled.csv\")\n",
    "df_scaled.to_csv(output_path, index=False)\n",
    "print(f\" Skalierte Variablen gespeichert unter: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476309fb-71da-434d-a65d-5113f4d2452c",
   "metadata": {},
   "source": [
    "### 3.3.2 Anzahl der Cluster bestimmen\n",
    "\n",
    "Um eine geeignete Anzahl an Clustern fÃ¼r die anschlieÃende Gruppierung der LÃ¤nder zu finden, werden zwei gÃ¤ngige Metriken angewendet: die Elbow-Methode (Inertia) und der Silhouette Score. Beide helfen dabei abzuschÃ¤tzen, bei welcher Clusteranzahl eine sinnvolle Balance zwischen HomogenitÃ¤t innerhalb der Cluster und TrennschÃ¤rfe zwischen den Clustern erreicht wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84344e17-5390-4bd0-8182-015c79fb9701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Auswahl (alle numerischen Spalten ohne Kontextspalten)\n",
    "context_cols = [\"Country Code\", \"Country Name\", \"Region\", \"IncomeGroup\"]\n",
    "feature_cols = [col for col in df_scaled.columns if col not in context_cols]\n",
    "\n",
    "X = df_scaled[feature_cols]\n",
    "\n",
    "# Cluster-Anzahl-Bereich\n",
    "k_range = range(2, 11)\n",
    "\n",
    "# Ergebnisse speichern\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow Plot\n",
    "ax[0].plot(k_range, inertias, marker=\"o\")\n",
    "ax[0].set_xlabel(\"Anzahl der Cluster (k)\")\n",
    "ax[0].set_ylabel(\"Inertia (Summe der quadrierten Distanzen)\")\n",
    "ax[0].set_title(\"Elbow-Methode\")\n",
    "\n",
    "# Silhouette Plot\n",
    "ax[1].plot(k_range, silhouette_scores, marker=\"o\", color=\"orange\")\n",
    "ax[1].set_xlabel(\"Anzahl der Cluster (k)\")\n",
    "ax[1].set_ylabel(\"Silhouette Score\")\n",
    "ax[1].set_title(\"Silhouette Scores\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, \"elbow_und_silhouette.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd79255-23d3-44d2-8f4a-f41b37889531",
   "metadata": {},
   "source": [
    "### 3.3.3 Clustering durchfÃ¼hren\n",
    "\n",
    "Die Elbow-Methode zeigt einen deutlichen Knick bei k=4, was auf eine sinnvolle Clusteranzahl hinweist. Auch der Silhouettenkoeffizient erreicht bei vier Clustern einen lokalen HÃ¶chstwert (ca. 0,39). Auf dieser Grundlage wurde fÃ¼r die weitere Analyse die Gruppierung in vier Cluster gewÃ¤hlt.\n",
    "\n",
    "Die Zuordnung zu den Clustern erfolgt nun mittels des KMeans-Algorithmus basierend auf den skalierten sozioÃ¶konomischen Variablen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e746d6-cdd5-493f-a002-1b87612b08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans-Modell initialisieren und fitten\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=\"auto\")\n",
    "df_scaled[\"cluster\"] = kmeans.fit_predict(df_scaled[CLUSTERING_VARIABLES])\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(\"Cluster-Zuweisung abgeschlossen.\")\n",
    "\n",
    "# Abspeichern\n",
    "output_path = os.path.join(RESULTS_PATH, \"dataset_clustered.csv\")\n",
    "df_scaled.to_csv(output_path, index=False)\n",
    "print(f\"Ergebnis gespeichert unter: {output_path}\")\n",
    "\n",
    "print(\"Anzahl der LÃ¤nder pro Cluster:\")\n",
    "print(df_scaled[\"cluster\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e3720-8823-4be5-a74f-c30bbeb2f7a9",
   "metadata": {},
   "source": [
    "Die Cluster sind unterschiedlich groÃ. Jedes Cluster in sich ist jedoch groÃ genug, um damit weitere Analysen machen zu kÃ¶nnen\n",
    "\n",
    "Bevor die einzelnen Cluster im Detail analysiert und interpretiert werden, erfolgt zunÃ¤chst eine Visualisierung der Clusterstruktur. Hierzu wurde eine Hauptkomponentenanalyse (PCA) durchgefÃ¼hrt, um die hochdimensionalen Daten auf zwei Dimensionen zu reduzieren. Die folgende Darstellung zeigt die rÃ¤umliche Verteilung der LÃ¤nder im PCA-Raum, farblich differenziert nach ihrer jeweiligen ClusterzugehÃ¶rigkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15052dfd-dd43-4b4a-8e7c-21f4f3dea719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hauptkomponentenanalyse (PCA) mit zwei Komponenten zur Visualisierung\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(df_scaled[feature_cols])\n",
    "df_scaled[\"PCA1\"], df_scaled[\"PCA2\"] = components[:,0], components[:,1]\n",
    "\n",
    "# Scatter-Plot zur Darstellung der Cluster im zweidimensionalen PCA-Raum\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in sorted(df_scaled[\"cluster\"].unique()):\n",
    "    subset = df_scaled[df_scaled[\"cluster\"] == cluster]\n",
    "    plt.scatter(subset[\"PCA1\"], subset[\"PCA2\"], label=f\"Cluster {cluster}\")\n",
    "plt.legend()\n",
    "plt.title(\"Cluster-Verteilung in PCA-Raum\")\n",
    "plt.xlabel(\"PCA1\")\n",
    "plt.ylabel(\"PCA2\")\n",
    "plt.savefig(os.path.join(RESULTS_PATH, \"pca_cluster.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af66aab-7884-46e3-a729-aaddce371b7f",
   "metadata": {},
   "source": [
    "Die Visualisierung im PCA-Raum zeigt eine klare Trennung der vier Cluster. Auch wenn es ein paar Ãberschneidungen gibt, bestÃ¤tigt die Projektion insgesamt die strukturelle Trennbarkeit der Gruppen im mehrdimensionalen Raum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b07c8-a08e-4e1d-8daa-98d15bf1c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesamter Silhouettenkoeffizient\n",
    "sil_score = silhouette_score(df_scaled[CLUSTERING_VARIABLES], df_scaled[\"cluster\"])\n",
    "print(f\"Silhouettenkoeffizient fÃ¼r k=4: {sil_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af000a-8261-40f4-a915-f54800e7c9c9",
   "metadata": {},
   "source": [
    "Der durchschnittliche Silhouettenkoeffizient fÃ¼r die gewÃ¤hlte Clustering-LÃ¶sung mit vier Gruppen betrÃ¤gt 0,387. Dieser Wert deutet auf eine ausreichende TrennschÃ¤rfe zwischen den Clustern hin und unterstÃ¼tzt die Entscheidung fÃ¼r k=4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5b0a8e-6d3e-4076-8618-627f2369df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster Zentren:\")\n",
    "print(pd.DataFrame(kmeans.cluster_centers_, columns=CLUSTERING_VARIABLES))\n",
    "\n",
    "# Verteilung der Einkommensgruppen in den Clustern\n",
    "print(\"Verteilung der Einkommensgruppen pro Cluster:\")\n",
    "print(pd.crosstab(df_scaled[\"cluster\"], df_scaled[\"IncomeGroup\"]), \"\\n\")\n",
    "\n",
    "# Verteilung der Weltregionen in den Clustern\n",
    "print(\"Verteilung der Weltregionen pro Cluster:\")\n",
    "print(pd.crosstab(df_scaled[\"cluster\"], df_scaled[\"Region\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b6085-9e1e-4c16-96b8-55f8731fc284",
   "metadata": {},
   "source": [
    "Beschreibung der Cluster\n",
    "\n",
    "\n",
    "Die vier ermittelten Cluster lassen sich inhaltlich gut voneinander abgrenzen:\n",
    "\n",
    "- Cluster 0: Wirtschaftlich und infrastrukturell gemischte LÃ¤nder mit hÃ¶herer Ungleichheit. Hierzu zÃ¤hlen viele lateinamerikanische und MENA-Staaten sowie einige transformierende Volkswirtschaften.\n",
    "- Cluster 1: Staaten mit niedrigem Entwicklungsstand, schwacher Infrastruktur und geringer Bildungsbeteiligung. Stark vertreten: LÃ¤nder mit niedrigem Einkommen, vor allem in Subsahara-Afrika und SÃ¼dasien.\n",
    "- - Cluster 2: Hoch entwickelte Staaten mit sehr hohem BIP, hoher Bildung, starker Digitalisierung und vergleichsweise geringer Ungleichheit. Dominieren: High-Income-Staaten, vor allem in Europa und Nordamerika.\n",
    "- Cluster 3: Bildungsstarke LÃ¤nder mit mittlerem Einkommen, solider Infrastruktur und ebenfalls relativ geringer Ungleichheit. AuffÃ¤llig viele Staaten aus Europa und Zentralasien.\n",
    "\n",
    "\n",
    "Diese Differenzierung bildet eine solide Grundlage fÃ¼r die anschlieÃende Analyse partizipativer Demokratie im Kontext struktureller Bedingungen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43823521-5219-410b-a395-f4a11a747a4c",
   "metadata": {},
   "source": [
    "## 3.4 Feature-Importance-Analyse innerhalb der Cluster\n",
    "\n",
    "ZunÃ¤chst wird ein Random-Forest-Modell auf dem Gesamtdatensatz trainiert, um die wichtigsten Einflussfaktoren auf die partizipative Demokratie zu identifizieren. AnschlieÃend erfolgt dieselbe Analyse fÃ¼r jedes Cluster separat, um Unterschiede im Einfluss struktureller Merkmale zwischen LÃ¤ndergruppen sichtbar zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e857ebb-96eb-4eec-a992-b0733102c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_cleaned.merge(df_scaled[[\"Country Code\", \"cluster\"]], on=\"Country Code\", how=\"left\")\n",
    "\n",
    "# Zielvariable & Kontext\n",
    "cluster_col = \"cluster\"\n",
    "\n",
    "# Features: alle numerischen Spalten auÃer der Zielvariable\n",
    "exclude_cols = [\"Country Code\", \"Country Name\", \"Region\", \"IncomeGroup\", TARGET_VARIABLE, cluster_col]\n",
    "feature_cols = [col for col in df_final.select_dtypes(include=[\"float64\", \"int64\"]).columns if col not in exclude_cols]\n",
    "\n",
    "# Globale Feature Importance (Ã¼ber alle LÃ¤nder hinweg)\n",
    "print(\"\\n Globale Feature Importance â Gesamtdatensatz\")\n",
    "X_all = df_final[feature_cols]\n",
    "y_all = df_final[TARGET_VARIABLE]\n",
    "\n",
    "rf_global = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_global.fit(X_all, y_all)\n",
    "\n",
    "global_importances = pd.DataFrame({\n",
    "    \"Feature\": feature_cols,\n",
    "    \"Importance\": rf_global.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    data=global_importances.head(15),\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    hue=\"Feature\",\n",
    "    palette=\"viridis\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Globale Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, \"globale_feature_importance.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Feature Importance fÃ¼r jeden Cluster\n",
    "for cluster in sorted(df_final[cluster_col].dropna().unique()):\n",
    "    print(f\"\\n Cluster {cluster} â LÃ¤nderanzahl: {df_final[df_final[cluster_col] == cluster].shape[0]}\")\n",
    "\n",
    "    # Daten aus Cluster extrahieren\n",
    "    df_cluster = df_final[df_final[cluster_col] == cluster]\n",
    "\n",
    "    # X und y\n",
    "    X = df_cluster[feature_cols]\n",
    "    y = df_cluster[TARGET_VARIABLE]\n",
    "\n",
    "    # Modell trainieren\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # Feature Importance extrahieren\n",
    "    importances = rf.feature_importances_\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"Feature\": feature_cols,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(\n",
    "    data=fi_df.head(15),\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    hue=\"Feature\",\n",
    "    palette=\"viridis\",\n",
    "    legend=False\n",
    ")\n",
    "    plt.title(f\"Top Feature Importances â Cluster {cluster}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, f\"feature_importance_{cluster}.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9c1f1-5ed0-4118-bf2c-7e35df299d0d",
   "metadata": {},
   "source": [
    "Die Visualisierungen der Feature-Importance-Analyse zeigen, dass sich die wichtigsten EinflussgrÃ¶Ãen auf partizipative Demokratie je nach Cluster deutlich unterscheiden.\n",
    "\n",
    "Im Gesamtdatensatz dominieren institutionelle Faktoren wie JustizunabhÃ¤ngigkeit (`v2x_jucon`) und Medienfreiheit (`v2mecenefi`) die ErklÃ¤rung partizipativer Demokratie. Auch gleichheitsbezogene Variablen wie `v2xeg_eqaccess` und `v2dlengage` sind zentral.\n",
    "\n",
    "In Cluster 0 (ungleiche, mittel entwickelte LÃ¤nder) ist Medienfreiheit der stÃ¤rkste Einflussfaktor, gefolgt von politischem Zugang und JustizunabhÃ¤ngigkeit. \n",
    "\n",
    "Cluster 1 (niedrig entwickelte LÃ¤nder) zeigt ein Ã¤hnliches Muster: Institutionelle Merkmale wie Justiz, Teilhabe und Gleichheit sind entscheidend, sozioÃ¶konomische Faktoren spielen kaum eine Rolle.\n",
    "\n",
    "In Cluster 2 (hoch entwickelte LÃ¤nder) gewinnen gleichheits- und vielfaltsbezogene Variablen an Bedeutung, wÃ¤hrend klassische Entwicklungsindikatoren moderat wichtig bleiben.\n",
    "\n",
    "Cluster 3 (bildungsstarke Mittelgruppe) wird ebenfalls stark durch Medienfreiheit, Gleichheit und zivilgesellschaftliche Beteiligung geprÃ¤gt, wÃ¤hrend Ã¶konomische Variablen geringere Relevanz aufweisen.\n",
    "\n",
    "Die Unterschiede in den Rangfolgen und den dominanten EinflussgrÃ¶Ãen legen nahe, dass partizipative Demokratie in verschiedenen Kontexten durch unterschiedliche Faktoren gefÃ¶rdert oder gehemmt wird."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
